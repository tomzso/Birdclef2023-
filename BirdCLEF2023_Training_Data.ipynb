{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GoUldEz6LT6U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout, ActivityRegularization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZJwKUqEcKJb1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Download the train, validation and test dataset which were created with the dataset preprocess files\n",
    "\n",
    "# file_id_audio = [\n",
    "#     '1NNxYN2DbuDF0tvzbZTQ_EPEJ5uIPZPai', # https://drive.google.com/file/d/1NNxYN2DbuDF0tvzbZTQ_EPEJ5uIPZPai/view?usp=sharing\n",
    "#     '1-CMJZY4allQBba5gl5s0Y-RT9Qdma_Mj', # https://drive.google.com/file/d/1-CMJZY4allQBba5gl5s0Y-RT9Qdma_Mj/view?usp=sharing\n",
    "#     '1--LAbXygf83JJuY-Do3tG_6cVxSErHr5', # https://drive.google.com/file/d/1--LAbXygf83JJuY-Do3tG_6cVxSErHr5/view?usp=sharing\n",
    "# ]\n",
    "\n",
    "\n",
    "# for i, id in enumerate(file_id_audio):\n",
    "#   # Check if the file already exists\n",
    "#   if (not os.path.exists('val_dataset.tfrecord')) or (not os.path.exists('train_dataset.tfrecord')) or (not os.path.exists('test_dataset.tfrecord')):\n",
    "#     !wget --content-disposition 'https://drive.google.com/uc?export=download&id={id}&confirm=t'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "PJ5wTbo2OWY1",
    "outputId": "944afcd4-ec91-40bf-ef5b-4fa5c1990656"
   },
   "outputs": [],
   "source": [
    "# # Another way to get the dataset from the Google Drive and the working enviroment is Google Colab \n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# drive_file_path = '/content/drive/MyDrive/test_dataset.tfrecord'\n",
    "# colab_file_path = '/content/test_dataset.tfrecord'\n",
    "\n",
    "# shutil.copy(drive_file_path, colab_file_path)\n",
    "\n",
    "# drive_file_path = '/content/drive/MyDrive/val_dataset.tfrecord'\n",
    "# colab_file_path = '/content/val_dataset.tfrecord'\n",
    "\n",
    "# shutil.copy(drive_file_path, colab_file_path)\n",
    "\n",
    "\n",
    "# drive_file_path = '/content/drive/MyDrive/train_dataset.tfrecord'\n",
    "# colab_file_path = '/content/train_dataset.tfrecord'\n",
    "\n",
    "# shutil.copy(drive_file_path, colab_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OU-PlhNPOjFG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse TFRecord entries, declaring the metadatas of the TFRecord files\n",
    "def _parse_function(proto):\n",
    "    keys_to_features = {\n",
    "        'data': tf.io.FixedLenFeature([128 * 313 * 3], tf.float32),\n",
    "        'labels': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
    "    parsed_features['data'] = tf.reshape(parsed_features['data'], (128, 313, 3))\n",
    "    return parsed_features['data'], parsed_features['labels']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2Rbs9i3IPhjM"
   },
   "outputs": [],
   "source": [
    "# Load and parse TFRecord datasets for testing, validation,and training\n",
    "# These dataset are pipelined so they are not loaded to the memory.\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset('test_dataset2.tfrecord').map(_parse_function)\n",
    "val_dataset = tf.data.TFRecordDataset('val_dataset2.tfrecord').map(_parse_function)\n",
    "train_dataset = tf.data.TFRecordDataset('train_dataset2.tfrecord').map(_parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7uKtkSaPrHn",
    "outputId": "1ae36241-b78e-4dd5-f5cc-a57c53b1611c"
   },
   "outputs": [],
   "source": [
    "# Create a ResNet model with a pre-trained ResNet50 base\n",
    "\n",
    "resnet_model = Sequential()\n",
    "pretrained_model= tf.keras.applications.VGG19(\n",
    "    include_top=False,\n",
    "    input_shape=(128,313,3),\n",
    "    pooling='avg',classes=264,\n",
    "    weights='imagenet')\n",
    "\n",
    "# Freeze pre-trained ResNet50 layers\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "\n",
    "resnet_model.add(pretrained_model)\n",
    "\n",
    "\n",
    "resnet_model.add(Flatten())\n",
    "\n",
    "resnet_model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "# # Add Dense layers with dropout and L2 regularization\n",
    "# resnet_model.add(Dense(1024, activation='relu',kernel_regularizer=keras.regularizers.l1(0.00001)))\n",
    "# resnet_model.add(BatchNormalization())\n",
    "# resnet_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# # Add Dense layers with dropout and L2 regularization\n",
    "# resnet_model.add(Dense(1024, activation='relu',kernel_regularizer=keras.regularizers.l1(0.00001)))\n",
    "# resnet_model.add(BatchNormalization())\n",
    "# resnet_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# resnet_model.add(Dense(512, activation='relu',kernel_regularizer=keras.regularizers.l1(0.00001)))\n",
    "# resnet_model.add(BatchNormalization())\n",
    "# resnet_model.add(Dropout(0.2))\n",
    "\n",
    "# resnet_model.add(Dense(1024, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001) ))\n",
    "# resnet_model.add(BatchNormalization())\n",
    "# resnet_model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "resnet_model.add(Dense(1024, activation='relu',kernel_regularizer=keras.regularizers.l2(0.0001)))\n",
    "resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "resnet_model.add(Dense(1024, activation='relu',kernel_regularizer=keras.regularizers.l2(0.0001)))\n",
    "resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Dropout(0.5))\n",
    "\n",
    "resnet_model.add(Dense(1024, activation='relu',kernel_regularizer=keras.regularizers.l2(0.0001)))\n",
    "resnet_model.add(BatchNormalization())\n",
    "resnet_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "resnet_model.add(Dense(264, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cicjbe4CPs9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 313, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 313, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 313, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 156, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 156, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 156, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 78, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 78, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 78, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 78, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 32, 78, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 39, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 39, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 39, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 39, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 16, 39, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 19, 512)        0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 19, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 19, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 19, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 8, 19, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "resnet_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "pretrained_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KhtRiTgXPu_x"
   },
   "outputs": [],
   "source": [
    "# Prefetch datasets for better performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle training dataset\n",
    "train_dataset = train_dataset.shuffle(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define early stopping and checkpoint callbacks\n",
    "patience = 20\n",
    "early_stopping=EarlyStopping(patience=patience, verbose=2)\n",
    "checkpointer=ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDdjRkzoP0jZ",
    "outputId": "aea2e7c8-a3f4-46d9-f0b3-9f2933ffca28",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "395/395 [==============================] - 346s 814ms/step - loss: 5.9420 - accuracy: 0.0503 - val_loss: 4.9893 - val_accuracy: 0.0837\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.98931, saving model to weights.hdf5\n",
      "Epoch 2/30\n",
      "395/395 [==============================] - 295s 746ms/step - loss: 5.1075 - accuracy: 0.1078 - val_loss: 4.5669 - val_accuracy: 0.1530\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.98931 to 4.56685, saving model to weights.hdf5\n",
      "Epoch 3/30\n",
      "395/395 [==============================] - 299s 757ms/step - loss: 4.7104 - accuracy: 0.1500 - val_loss: 4.4038 - val_accuracy: 0.1791\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.56685 to 4.40381, saving model to weights.hdf5\n",
      "Epoch 4/30\n",
      "395/395 [==============================] - 312s 788ms/step - loss: 4.4751 - accuracy: 0.1836 - val_loss: 4.3139 - val_accuracy: 0.1918\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.40381 to 4.31394, saving model to weights.hdf5\n",
      "Epoch 5/30\n",
      "395/395 [==============================] - 299s 756ms/step - loss: 4.3037 - accuracy: 0.2078 - val_loss: 4.2922 - val_accuracy: 0.2015\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.31394 to 4.29220, saving model to weights.hdf5\n",
      "Epoch 6/30\n",
      "395/395 [==============================] - 311s 788ms/step - loss: 4.1787 - accuracy: 0.2323 - val_loss: 4.2556 - val_accuracy: 0.2105\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.29220 to 4.25560, saving model to weights.hdf5\n",
      "Epoch 7/30\n",
      "395/395 [==============================] - 302s 764ms/step - loss: 4.0883 - accuracy: 0.2471 - val_loss: 4.2301 - val_accuracy: 0.2166\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.25560 to 4.23013, saving model to weights.hdf5\n",
      "Epoch 8/30\n",
      "395/395 [==============================] - 327s 823ms/step - loss: 4.0160 - accuracy: 0.2610 - val_loss: 4.2169 - val_accuracy: 0.2216\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.23013 to 4.21694, saving model to weights.hdf5\n",
      "Epoch 9/30\n",
      "395/395 [==============================] - 300s 759ms/step - loss: 3.9473 - accuracy: 0.2762 - val_loss: 4.2468 - val_accuracy: 0.2217\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.21694\n",
      "Epoch 10/30\n",
      "395/395 [==============================] - 299s 756ms/step - loss: 3.9074 - accuracy: 0.2858 - val_loss: 4.2231 - val_accuracy: 0.2282\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.21694\n",
      "Epoch 11/30\n",
      "395/395 [==============================] - 300s 760ms/step - loss: 3.8758 - accuracy: 0.2956 - val_loss: 4.2579 - val_accuracy: 0.2307\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.21694\n",
      "Epoch 12/30\n",
      "395/395 [==============================] - 294s 743ms/step - loss: 3.8269 - accuracy: 0.3062 - val_loss: 4.2473 - val_accuracy: 0.2322\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.21694\n",
      "Epoch 13/30\n",
      "381/395 [===========================>..] - ETA: 8s - loss: 3.8123 - accuracy: 0.3124"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "history = resnet_model.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset.batch(batch_size),\n",
    "    callbacks=[checkpointer, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# num_epochs = 30\n",
    "# batch_size = 256\n",
    "\n",
    "# history = resnet_model.fit(\n",
    "#     train_dataset.batch(batch_size),\n",
    "#     epochs=num_epochs,\n",
    "#     validation_data=val_dataset.batch(batch_size),\n",
    "#     #callbacks=[checkpointer, early_stopping]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the accuracy on the test dataset\n",
    "\n",
    "# evaluation_result = resnet_model.evaluate(test_dataset.batch(batch_size))\n",
    "\n",
    "# loss_value = evaluation_result[0]\n",
    "\n",
    "# accuracy_value = evaluation_result[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'the test accuracy is {accuracy_value}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
