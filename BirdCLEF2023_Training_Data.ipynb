{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GoUldEz6LT6U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout, ActivityRegularization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZJwKUqEcKJb1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Download the train, validation and test dataset from google colab which were created with the dataset preprocess files\n",
    "\n",
    "dataset = [\n",
    "    '1-IpL9lR5gEFMKGLfDaIIPlrzf2zfnGaN', #https://drive.google.com/file/d/1-IpL9lR5gEFMKGLfDaIIPlrzf2zfnGaN/view?usp=sharing\n",
    "    '1-IilI3zbFwBWIRmi4VvKkI6GkHJ6hXwQ', #https://drive.google.com/file/d/1-IilI3zbFwBWIRmi4VvKkI6GkHJ6hXwQ/view?usp=sharing\n",
    "    '1kKCM-H7cgHTw5p0wQsyNH2Ali-30Bbz-', #https://drive.google.com/file/d/1kKCM-H7cgHTw5p0wQsyNH2Ali-30Bbz-/view?usp=sharing\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Download the dataset audio files\n",
    "for i, id in enumerate(dataset):\n",
    "  destination = f'{i}.wav'\n",
    "\n",
    "  # Check if the file already exists\n",
    "  if not os.path.exists(destination):\n",
    " \n",
    "    !wget --content-disposition 'https://drive.google.com/uc?export=download&id={id}&confirm=t'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "PJ5wTbo2OWY1",
    "outputId": "944afcd4-ec91-40bf-ef5b-4fa5c1990656"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OU-PlhNPOjFG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse TFRecord entries, declaring the metadatas of the TFRecord files\n",
    "def _parse_function(proto):\n",
    "    keys_to_features = {\n",
    "        'data': tf.io.FixedLenFeature([128 * 313 * 3], tf.float32),\n",
    "        'labels': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n",
    "    parsed_features['data'] = tf.reshape(parsed_features['data'], (128, 313, 3))\n",
    "    return parsed_features['data'], parsed_features['labels']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2Rbs9i3IPhjM"
   },
   "outputs": [],
   "source": [
    "# Load and parse TFRecord datasets for testing, validation,and training\n",
    "# These dataset are pipelined so they are not loaded to the memory.\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset('test_dataset.tfrecord').map(_parse_function)\n",
    "val_dataset = tf.data.TFRecordDataset('val_dataset.tfrecord').map(_parse_function)\n",
    "train_dataset = tf.data.TFRecordDataset('train_dataset.tfrecord').map(_parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7uKtkSaPrHn",
    "outputId": "1ae36241-b78e-4dd5-f5cc-a57c53b1611c"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "pretrained_model= tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    input_shape=(128,313,3),\n",
    "    pooling='avg',classes=264,\n",
    "    weights='imagenet')\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "\n",
    "\n",
    "model.add(pretrained_model)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(600, activation='relu',kernel_regularizer=keras.regularizers.l2(0.00001)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(600, activation='relu',kernel_regularizer=keras.regularizers.l2(0.00001)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(264, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cicjbe4CPs9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 313, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 313, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 313, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 156, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 156, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 156, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 78, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 78, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 78, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 78, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 39, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 39, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 39, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 39, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 19, 512)        0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 19, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 19, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 19, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "pretrained_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KhtRiTgXPu_x"
   },
   "outputs": [],
   "source": [
    "# Prefetch datasets for better performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle training dataset\n",
    "train_dataset = train_dataset.shuffle(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define early stopping and checkpoint callbacks\n",
    "patience = 5\n",
    "early_stopping=EarlyStopping(patience=patience, verbose=2)\n",
    "checkpointer=ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDdjRkzoP0jZ",
    "outputId": "aea2e7c8-a3f4-46d9-f0b3-9f2933ffca28",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "202/202 [==============================] - 177s 774ms/step - loss: 5.0852 - accuracy: 0.0614 - val_loss: 5.3161 - val_accuracy: 0.0398\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.31608, saving model to weights.hdf5\n",
      "Epoch 2/30\n",
      "111/202 [===============>..............] - ETA: 48s - loss: 4.6145 - accuracy: 0.1101"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset.batch(batch_size),\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset.batch(batch_size),\n",
    "    callbacks=[checkpointer, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download weights if needed\n",
    "\n",
    "!wget --content-disposition 'https://drive.google.com/uc?export=download&id=16nsgJFzI7bIwovAvIVJNNNOpXL6B3abM&confirm=t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign to variable dense_11/kernel:0 due to variable shape (1024, 512) and value shape (1024, 1024) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Shuffle training dataset\u001b[39;00m\n\u001b[0;32m      6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     12\u001b[0m evaluation_result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_dataset\u001b[38;5;241m.\u001b[39mbatch(batch_size))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:2361\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2358\u001b[0m       hdf5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group_by_name(\n\u001b[0;32m   2359\u001b[0m           f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch)\n\u001b[0;32m   2360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2361\u001b[0m       \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2363\u001b[0m \u001b[38;5;66;03m# Perform any layer defined finalization of the layer state.\u001b[39;00m\n\u001b[0;32m   2364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\saving\\hdf5_format.py:713\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer #\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(k) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    706\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the current model) was found to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    707\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrespond to layer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in the save file. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    710\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m weights, but the saved weights have \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    711\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weight_values)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m elements.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    712\u001b[0m   weight_value_tuples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(symbolic_weights, weight_values)\n\u001b[1;32m--> 713\u001b[0m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_value_tuples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py:3775\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions():\n\u001b[0;32m   3774\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m x, value \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[1;32m-> 3775\u001b[0m     \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3777\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
      "File \u001b[1;32m~\\.conda\\envs\\env2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:899\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    897\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     tensor_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 899\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    900\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign to variable\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m due to variable shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    902\u001b[0m       (tensor_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, value_tensor\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    903\u001b[0m assign_op \u001b[38;5;241m=\u001b[39m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39massign_variable_op(\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, value_tensor, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_value:\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot assign to variable dense_11/kernel:0 due to variable shape (1024, 512) and value shape (1024, 1024) are incompatible"
     ]
    }
   ],
   "source": [
    "# Prefetch datasets for better performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle training dataset\n",
    "train_dataset = train_dataset.shuffle(5)\n",
    "\n",
    "\n",
    "model.load_weights('weights.hdf5')\n",
    "batch_size = 128\n",
    "\n",
    "evaluation_result = model.evaluate(test_dataset.batch(batch_size))\n",
    "\n",
    "loss_value = evaluation_result[0]\n",
    "accuracy_value = evaluation_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'the test accuracy is {accuracy_value}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
